{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import itcl_quantizer\n",
    "from itcl_quantizer import keras_build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = np.load(\"data/x_test.npy\"), np.load(\"data/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle x_test and y_test\n",
    "idx = np.arange(x_test.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"models/high_complexity_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"high_complexity_relu\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 16, 32)            4736      \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 16, 64)            24832     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 16, 128)           98816     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 264,129\n",
      "Trainable params: 264,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:lstm_5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model models/high_complexity_model.h5 with 6 layers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "MinMaxList: Unexpected type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m keras_build(\u001b[39m\"\u001b[39;49m\u001b[39mmodels/high_complexity_model.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtmp.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m, x_test[:\u001b[39m100\u001b[39;49m])\n",
      "File \u001b[1;32mD:\\codebases\\itcl\\itcl-quantization-toolkit\\quantizer\\itcl_quantizer\\tensor_extractor\\keras\\keras_builder.py:77\u001b[0m, in \u001b[0;36mbuild\u001b[1;34m(model_path, output_path, representative_input, loss_fn, cfg)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m# get layers:\u001b[39;00m\n\u001b[0;32m     75\u001b[0m layers \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers\n\u001b[1;32m---> 77\u001b[0m quantized \u001b[39m=\u001b[39m _quantize(layers, representative_input, cfg)\n\u001b[0;32m     78\u001b[0m network \u001b[39m=\u001b[39m Network(quantized)\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m loss_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     \u001b[39m# ParamEqualizerNet(network, loss_fn, lambda: ParamEqAnnealer(0.1, 0.1, steps=2000)).equalize()\u001b[39;00m\n",
      "File \u001b[1;32mD:\\codebases\\itcl\\itcl-quantization-toolkit\\quantizer\\itcl_quantizer\\tensor_extractor\\keras\\keras_builder.py:45\u001b[0m, in \u001b[0;36m_quantize\u001b[1;34m(layers, representative_input, cfg)\u001b[0m\n\u001b[0;32m     42\u001b[0m     previous \u001b[39m=\u001b[39m sequential_quantized[i]\n\u001b[0;32m     43\u001b[0m     layer_q \u001b[39m=\u001b[39m get_layer_quantizer(layer, cfg)\n\u001b[1;32m---> 45\u001b[0m     quantized_op \u001b[39m=\u001b[39m layer_q\u001b[39m.\u001b[39;49mquantize(previous)\n\u001b[0;32m     46\u001b[0m     sequential_quantized\u001b[39m.\u001b[39mappend(quantized_op)\n\u001b[0;32m     48\u001b[0m \u001b[39m# add the Dequantize Layer:\u001b[39;00m\n",
      "File \u001b[1;32mD:\\codebases\\itcl\\itcl-quantization-toolkit\\quantizer\\itcl_quantizer\\tensor_extractor\\keras\\layers\\keras_lstm.py:133\u001b[0m, in \u001b[0;36mKerasLSTM.quantize\u001b[1;34m(self, input_result)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m input_node \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLSTM: Input Node is None, the layer expects previous data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m     )\n\u001b[1;32m--> 133\u001b[0m lstm_states \u001b[39m=\u001b[39m LSTM(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_layer)\u001b[39m.\u001b[39;49mcall(input_data)\n\u001b[0;32m    135\u001b[0m recurrent_add \u001b[39m=\u001b[39m lstm_states\u001b[39m.\u001b[39mrecurrent_add\n\u001b[0;32m    136\u001b[0m bias_add \u001b[39m=\u001b[39m lstm_states\u001b[39m.\u001b[39mbias_add\n",
      "File \u001b[1;32mD:\\codebases\\itcl\\itcl-quantization-toolkit\\quantizer\\itcl_quantizer\\tensor_extractor\\keras\\layers\\keras_lstm.py:298\u001b[0m, in \u001b[0;36mLSTM.call\u001b[1;34m(self, window_batch)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, window_batch: NPFP32) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LSTMResult:\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(window_batch)\n",
      "File \u001b[1;32mD:\\codebases\\itcl\\itcl-quantization-toolkit\\quantizer\\itcl_quantizer\\tensor_extractor\\keras\\layers\\keras_lstm.py:343\u001b[0m, in \u001b[0;36mLSTM.__call__\u001b[1;34m(self, window_batch)\u001b[0m\n\u001b[0;32m    340\u001b[0m previous_recurrent_add\u001b[39m.\u001b[39mappend(Y)\n\u001b[0;32m    342\u001b[0m i \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msigmoid(Y[:, :n_units])\n\u001b[1;32m--> 343\u001b[0m previous_i\u001b[39m.\u001b[39;49mappend(i)\n\u001b[0;32m    345\u001b[0m f \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msigmoid(Y[:, n_units : \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m n_units])\n\u001b[0;32m    346\u001b[0m previous_f\u001b[39m.\u001b[39mappend(f)\n",
      "File \u001b[1;32mD:\\codebases\\itcl\\itcl-quantization-toolkit\\quantizer\\itcl_quantizer\\tensor_extractor\\keras\\layers\\keras_lstm.py:268\u001b[0m, in \u001b[0;36mMinMaxList.append\u001b[1;34m(self, _MinMaxList__object)\u001b[0m\n\u001b[0;32m    266\u001b[0m         __object \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(__object), \u001b[39mmax\u001b[39m(__object)\n\u001b[0;32m    267\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMinMaxList: Unexpected type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mappend(__object)\n",
      "\u001b[1;31mValueError\u001b[0m: MinMaxList: Unexpected type"
     ]
    }
   ],
   "source": [
    "keras_build(\"models/high_complexity_model.h5\", \"tmp.h5\", x_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f3c92647f29f0e33d74b4f49d79fbd7f977247a228e241a26f617e3907f3314"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
